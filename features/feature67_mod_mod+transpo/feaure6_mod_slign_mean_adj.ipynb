{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19585,"status":"ok","timestamp":1683915767919,"user":{"displayName":"Fan Zhou","userId":"13146613923931407945"},"user_tz":-120},"id":"eENvmswJkUqF","outputId":"406d667c-655d-4209-a6d3-07c757928ea8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LB6AlBR6UvwR"},"outputs":[],"source":["!pip install spacy\n","!python -m spacy download en_core_web_sm\n","!pip install jieba\n","!pip install nltk\n","!pip install transformers\n","!pip install sentencepiece\n","!pip install stanza\n","!pip install -U synonyms\n","!python -c \"import synonyms\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25VqbshxU58Z"},"outputs":[],"source":["import spacy\n","from jieba import posseg\n","from nltk.corpus import wordnet \n","import jieba\n","import nltk\n","import stanza\n","from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n","from transformers import BertTokenizer, BertModel\n","import re\n","import synonyms\n","import torch\n","from torch.nn.functional import cosine_similarity\n","\n","nltk.download('wordnet')\n","nlp_en = spacy.load(\"en_core_web_sm\")\n","\n","\n","model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n","tokenizer_zh = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"zh_CN\")\n","tokenizer_en = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"en_XX\")\n","tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAw-GjpiU9h8"},"outputs":[],"source":["class Modtrans:\n","\n","    def __init__(self,path,save_path):\n","        with open(path) as f:\n","          text = f.readlines()\n","        self.en_li = []\n","        self.zh_li = []\n","        for i, sent in enumerate(text):\n","            if i%2 == 0:\n","                self.en_li.append(sent.strip())\n","            elif i%2 == 1:\n","                self.zh_li.append(sent.strip())\n","        self.save_path = save_path\n","\n","\n","    def without_transp(self, en_pos = 'ADJ', zh_pos = 'a'):  # adj,  verb\n","        e = 151793\n","\n","        for en,zh in zip(self.en_li[151793:], self.zh_li[151793:]):\n","            # en_verb_li = [tok_en.text for tok_en in nlp_en(en) if tok_en.pos_ == 'VERB' and tok_en.text.endswith(\"ing\") and 'go' not in en and 'get' not in en and 'take' not in en]\n","            en_verb_li = [tok_en.text for tok_en in nlp_en(en) if tok_en.pos_ == en_pos]\n","            en_all_li = [tok_en.text for tok_en in nlp_en(en)]\n","            print(e)\n","            e += 1\n","            \n","            if en_verb_li != []:\n","                zh_verb_li = []\n","                for zh_tok,label in posseg.cut(zh):\n","                    if label == zh_pos:\n","                        zh_verb_li.append(zh_tok)\n","                \n","                if zh_verb_li != []:\n","                    en2zh_verb_li = []\n","                    for en_verb in en_verb_li:\n","                        en_text_trans = self.en2zh(en_verb)\n","                        en2zh_verb_li.append(en_text_trans)\n","                    tup_li = []\n","                    for i, en2zh_verb in enumerate(en2zh_verb_li):\n","                        for zh_verb in zh_verb_li:\n","                            if len(en2zh_verb.split()) == 1 and len(zh_verb.split()) == 1:\n","                                zh_text_trans =self.zh2en(zh_verb)\n","\n","\n","\n","                                # if zh_text_trans in \n","                                r = synonyms.compare(en2zh_verb,zh_verb, seg=False)\n","                                if r > 0.5:\n","                                    break\n","                                elif r >= 0.2 and r <= 0.5:\n","                                    if len(list(set(en2zh_verb).intersection(set(zh)))) == 0:\n","                                        tup = (en_verb_li[i],en2zh_verb,zh_verb,r)\n","                                        tup_li.append(tup)\n","                                    \n","\n","                    for t in list(set(tup_li)):\n","                        print(en)\n","                        print(zh)                      \n","                        print(t)\n","                        print('--------')\n","                        with open(self.save_path, 'a+') as w:\n","                            w.write(en+'\\n'+zh+'\\n'+t[0]+':'+t[1]+':'+t[2]+'\\n'+'\\n')\n","\n","    def with_transp(self, en_pos = 'ADJ', zh_pos = 'a'):  # adj,  verb\n","        e = 1\n","\n","        for en,zh in zip(self.en_li, self.zh_li):\n","            # en_verb_li = [tok_en.text for tok_en in nlp_en(en) if tok_en.pos_ == 'VERB' and tok_en.text.endswith(\"ing\") and 'go' not in en and 'get' not in en and 'take' not in en]\n","            en_verb_li = [tok_en.text for tok_en in nlp_en(en) if tok_en.pos_ == en_pos]\n","            en_all_li = [tok_en.text for tok_en in nlp_en(en)]\n","            print(e)\n","            e += 1\n","            if en_verb_li != []:\n","                zh_verb_li = []\n","                for zh_tok,label in posseg.cut(zh):\n","                    if label == zh_pos:\n","                        zh_verb_li.append(zh_tok)\n","                \n","                if zh_verb_li != []:\n","                    en2zh_verb_li = []\n","                    for en_verb in en_verb_li:\n","                        en_text_trans = self.en2zh(en_verb)\n","                        en2zh_verb_li.append(en_text_trans)\n","                    tup_li = []\n","                    for i, en2zh_verb in enumerate(en2zh_verb_li):\n","                        for zh_verb in zh_verb_li:\n","                            if len(en2zh_verb.split()) == 1 and len(zh_verb.split()) == 1:\n","                                zh_text_trans =self.zh2en(zh_verb)\n","\n","\n","\n","                                # if zh_text_trans in \n","                                r = synonyms.compare(en2zh_verb,zh_verb, seg=False)\n","                                if r > 0.4:\n","                                    break\n","                                elif r >= 0.2 and r <= 0.5:\n","                                    if len(list(set(en2zh_verb).intersection(set(zh)))) == 0:\n","                                        tup = (en_verb_li[i],en2zh_verb,zh_verb,r)\n","                                        tup_li.append(tup)\n","                                    \n","\n","                    for t in list(set(tup_li)):\n","                        # print(en)\n","                        # print(zh)                      \n","                        # print(t)\n","                        # print('--------')\n","                        with open(self.save_path, 'a+') as w:\n","                            w.write(en+'\\n'+zh+'\\n'+t[0]+':'+t[1]+':'+t[2]+'\\n'+'\\n')\n","\n","\n","    def en2zh(self,word):\n","        en_token = tokenizer_en(word, return_tensors=\"pt\")\n","        generated_tokens = model.generate(**en_token, forced_bos_token_id=tokenizer_en.lang_code_to_id[\"zh_CN\"])\n","        en_text_trans = tokenizer_en.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n","        return en_text_trans\n","    \n","\n","\n","\n","    def zh2en(self,word):\n","        en_token = tokenizer_zh(word, return_tensors=\"pt\")\n","        generated_tokens = model.generate(**en_token, forced_bos_token_id=tokenizer_zh.lang_code_to_id[\"en_XX\"])\n","        zh_text_trans = tokenizer_zh.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n","        return zh_text_trans\n","\n","\n","\n","    def en_zh_similarity(self,word):\n","        inputs = tokenizer(word, return_tensors=\"pt\")\n","        outputs = model(**inputs)\n","        embedding = outputs.encoder_last_hidden_state.mean(dim=1)\n","        return embedding\n","\n","\n","\n","    def easy_way_mod(self, en_pos = 'ADJ', zh_pos = 'a'):\n","        for en,zh in zip(self.en_li, self.zh_li):\n","            en_verb_li = [tok_en.text for tok_en in nlp_en(en) if tok_en.pos_ == en_pos]\n","            if en_verb_li != []:\n","                zh_verb_li = []\n","                for zh_tok,label in posseg.cut(zh):\n","                    if label == zh_pos:\n","                        zh_verb_li.append(zh_tok)\n","\n","\n","                if zh_verb_li != []:\n","                    tup_li = []\n","                    for en_verb in en_verb_li:\n","                        for zh_verb in zh_verb_li:\n","                            if len(zh_verb.split()) == 1 and len(zh_verb.split()) == 1:\n","                                emb_en = self.en_zh_similarity(en_verb)\n","                                emb_zh = self.en_zh_similarity(zh_verb)\n","                                similarity = cosine_similarity(emb_en, emb_zh)\n","                                tup = (en_verb,zh_verb,similarity.item())\n","                                print(en)\n","                                print(zh)\n","                                print(tup)\n","\n","\n","\n","                  \n","\n","\n","\n","        \n","            \n","spath = '/content/gdrive/MyDrive/Colab Notebooks/literal translationese/dataset/Bilingual/Education/Bi-Education.txt'\n","save_path = '/content/gdrive/MyDrive/Colab Notebooks/literal translationese/dataset/feature67_mod_mod+transpo/mod_slight_mean_change_adj_new.txt'             \n","        \n","ptcl = Modtrans(spath, save_path)\n","\n","ptcl.without_transp()\n","\n","\n"]}],"metadata":{"colab":{"machine_shape":"hm","toc_visible":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}