{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36923,"status":"ok","timestamp":1684058250084,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"},"user_tz":-120},"id":"56LoHABo3x4K","outputId":"1e137407-2a93-4fd1-c5b0-a7fffa55bd5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wt3g50f_yCcp"},"outputs":[],"source":["# !pip3 install idiomatch\n","!pip3 install spacy\n","!python3 -m spacy download en_core_web_sm\n","!pip install jieba\n","!pip install nltk\n","!pip install transformers\n","!pip install sentencepiece\n","!pip install -U synonyms\n","!python -c \"import synonyms\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DlbAh7Fc0b0q"},"outputs":[],"source":["import spacy\n","# from idiomatch import Idiomatcher\n","nlp_en = spacy.load(\"en_core_web_sm\")\n","import synonyms\n","from jieba import posseg\n","# idiomatcher = Idiomatcher.from_pretrained(nlp_en)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1uYCDIS1TKv"},"outputs":[],"source":["from nltk.corpus import wordnet \n","import jieba\n","import nltk\n","nltk.download('wordnet')\n","\n","from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n","\n","model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n","tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"zh_CN\")\n","tokenizer_en = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"en_XX\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHw7JX8M2P50"},"outputs":[],"source":["class Generalization:\n","\n","    def __init__(self,path,save_path):\n","        with open(path) as f:\n","          text = f.readlines()\n","        self.en_li = []\n","        self.zh_li = []\n","        for i, sent in enumerate(text):\n","            if i%2 == 0:\n","                self.en_li.append(sent.strip())\n","            elif i%2 == 1:\n","                self.zh_li.append(sent.strip())\n","        self.save_path = save_path\n","\n","    def idiom(self):\n","        e = 1\n","        for en,zh in zip(self.en_li, self.zh_li):\n","            doc = nlp_en(sent)\n","            en_idiom = idiomatcher.identify(doc)\n","            if en_idiom != []:\n","                print(en)\n","                print(zh)\n","                print(en_idiom)\n","\n","\n","    def pronoun(self):\n","        r = 1\n","        for en,zh in zip(self.en_li[11000:], self.zh_li[11000:]):\n","            if '他们' in zh and 'hey' not in en and 'heir' not in en and 'them' not in en and 'who' not in en and 'that' not in en: #and '所有' not in zh and '全部' not in zh and '全' not in zh and '都' not in zh and '一' not in zh and 'at all' not in en: #and \"it's\" not in en and \"it comes\" not in en and \"that\" not in en:\n","                # if en_tok_li.index('it') != 0 and \"\".join(tok.pos_ for tok in nlp_en(en_tok_li[en_tok_li.index('it')-1])) in ['VERB','ADV']:\n","                    \n","                seg_zh = posseg.cut(zh)\n","                noun_list = [tok.text for tok in nlp_en(en) if tok.pos_ in ['NOUN'] and tok.text != tok.lemma_]\n","                if noun_list != []:\n","                    # print(en)\n","                    # print(zh)\n","                    # print(noun_list)\n","                    # print()\n","                    with open(save_path,'a+') as q:\n","                        q.write(en+'\\n'+zh+'\\n'+str(noun_list)+'\\n'+'他们:'+'\\n')    ####这里要改！！！！！！！！！！！\n","                        print(r)\n","                        r += 1\n","\n","    def hypernym(self):\n","        r = 1\n","        for en,zh in zip(self.en_li, self.zh_li):\n","            en_tok_li = [tok.text for tok in nlp_en(en)]\n","            zh_tok_li = jieba.cut(zh)\n","            zh2en_dict = {}\n","            for tok_zh in zh_tok_li:\n","                en_token = tokenizer(tok_zh, return_tensors=\"pt\")\n","                generated_tokens = model.generate(**en_token, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n","                zh_text_trans = [tok.lemma_ for tok in nlp_en(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])][0]\n","                # print(zh_text_trans)\n","                zh2en_dict[zh_text_trans] = tok_zh\n","            \n","            print(r)\n","            r+=1\n","            for en_tok in en_tok_li:\n","                \n","                if wordnet.synsets(en_tok) != []:\n","                    # print(en_tok)\n","                    hypernyms = wordnet.synsets(en_tok)[0].hypernyms()\n","                    hyper_li = [h.lemma_names() for h in hypernyms]\n","                    if hyper_li != []:\n","                        hyper_li_final = []\n","                        hyper_str = '_'.join(hyper_li[0])\n","                        for ph in hyper_li[0]:\n","                            if '_' in ph:\n","                                new_ph = ph.replace('_',' ')\n","                                hyper_li_final.append(new_ph)\n","                            else:\n","                                hyper_li_final.append(ph)\n","                                \n","                    \n","                        for en2zh in zh2en_dict.keys():\n","                            if en2zh in hyper_li_final:\n","                                # print(en)\n","                                # print(zh)\n","                                # print(en_tok, zh2en_dict[en2zh])\n","                                # print()\n","                                with open(save_path,'a+') as q:\n","                                    q.write(en+'\\n'+zh+'\\n'+en_tok+':'+zh2en_dict[en2zh]+'\\n')\n","\n","\n","\n","    def hyponym(self):\n","        r = 1\n","        for en,zh in zip(self.en_li[142000:], self.zh_li[142000:]):\n","            en_tok_li = [tok.text for tok in nlp_en(en)]\n","            zh_tok_li = jieba.cut(zh)\n","            zh2en_dict = {}\n","            for tok_zh in zh_tok_li:\n","                en_token = tokenizer(tok_zh, return_tensors=\"pt\")\n","                generated_tokens = model.generate(**en_token, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n","                zh_text_trans = [tok.lemma_ for tok in nlp_en(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])][0]\n","                # print(zh_text_trans)\n","                zh2en_dict[zh_text_trans] = tok_zh\n","            \n","            print(r)\n","            r+=1\n","            for en_tok in en_tok_li:\n","                # print(en_tok)\n","                \n","                if wordnet.synsets(en_tok) != []:\n","                    # print(en_tok)\n","                    hyponyms = wordnet.synsets(en_tok)[0].hyponyms()\n","                    hypo_li = [h.lemma_names() for h in hyponyms]\n","                    if hypo_li != []:\n","                        hypo_li_final = []\n","                        for ph in hypo_li[0]:\n","                            if '_' in ph:\n","                                new_ph = ph.replace('_',' ')\n","                                hypo_li_final.append(new_ph)\n","                            else:\n","                                hypo_li_final.append(ph)\n","                        # print(hypo_li_final)\n","                                \n","                    \n","                        for en2zh in zh2en_dict.keys():\n","                            if en2zh in hypo_li_final:\n","                                # print(en)\n","                                # print(zh)\n","                                # print(en_tok, zh2en_dict[en2zh])\n","                                # print()\n","                                with open(save_path,'a+') as q:\n","                                    q.write(en+'\\n'+zh+'\\n'+en_tok+':'+zh2en_dict[en2zh]+'\\n')\n","\n","\n","\n","\n","\n","\n","\n","spath = '/content/gdrive/MyDrive/Colab Notebooks/literal translationese/dataset/Bilingual/News/Bi-News.txt'\n","save_path = '/content/gdrive/MyDrive/Colab Notebooks/literal translationese/dataset/feature45_particular_generalization/hyponym_News_new.txt'             \n","        \n","ptcl = Generalization(spath, save_path)\n","\n","ptcl.hyponym()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZ3RbJVNbL-H"},"outputs":[],"source":["class Cleaning:\n","    def __init__(self,path,save_path,n):\n","        with open(path) as f:\n","          text = f.readlines()\n","        self.en_li = []\n","        self.zh_li = []\n","        self.align_li = []\n","        self.n = n\n","        for i, sent in enumerate(text):\n","            if i%self.n == 0:\n","                self.en_li.append(sent.strip())\n","            elif i%self.n == 1:\n","                self.zh_li.append(sent.strip())\n","            elif i%self.n == 2:\n","                self.align_li.append(sent.strip())\n","        self.save_path = save_path\n","\n","    def clean_duplicate(self):\n","\n","        for en,zh,align_pair in zip(self.en_li, self.zh_li, self.align_li):\n","            en_part = align_pair.split(':')[0]\n","            zh_part = align_pair.split(':')[-1]\n","\n","            zh_tok_li = jieba.cut(zh)\n","            zh_tok_li_rest = [zh_tok for zh_tok in zh_tok_li if zh_tok != zh_part]\n","\n","            en_token = tokenizer_en(en_part, return_tensors=\"pt\")\n","            generated_tokens = model.generate(**en_token, forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"])\n","            zh_text_trans = tokenizer_en.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n","            # print(en_part)\n","            # print(zh_text_trans)\n","\n","            if len(zh_part) != 0 and len(zh_text_trans) != 0:\n","                score = synonyms.compare(zh_part,zh_text_trans,seg=True)\n","                if score >=0.7:\n","                    real_align_li = []\n","                    for zh_tok_r in zh_tok_li_rest:\n","\n","                        if len(zh_text_trans) !=0 and len(zh_tok_r) != 0:\n","                            r = synonyms.compare(zh_text_trans,zh_tok_r,seg=True)\n","                            if zh_text_trans not in zh and r <= 0.7 and len(zh_part)>1 and '妈' not in zh and 'specif' not in en:\n","                                real_align = en_part+':'+zh_part\n","                                real_align_li.append(real_align)\n","                    for real in list(set(real_align_li)):\n","                        print(en)\n","                        print(zh)\n","                        print(real)\n","                        with open(self.save_path,'a+') as y:\n","                          y.write(en+'\\n'+zh+'\\n'+real+'\\n')\n","\n","                        \n","\n","\n","\n","\n","\n","                \n","\n","\n","spath = '/content/gdrive/MyDrive/Colab Notebooks/literal translationese/dataset/feature45_particular_generalization/to_be_cleaned/Subtitles.txt'\n","save_path = '/content/gdrive/MyDrive/Colab Notebooks/literal translationese/dataset/feature45_particular_generalization/to_be_cleaned/Subtitles_new.txt'             \n","        \n","ptcl = Cleaning(spath, save_path,3)\n","\n","ptcl.clean_duplicate()\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1ngGbvMN6YLffJcaEdBG_YCCRzG7rEhtW","timestamp":1682512839981},{"file_id":"1RNr41I27r8LFBDaOBM3mUYrI6VayXleX","timestamp":1682501451155}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}